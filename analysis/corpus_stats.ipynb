{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gh0V0BIjVPoI",
    "outputId": "6a8f9a1d-6aad-46a7-ef4a-ea97bda44a09"
   },
   "outputs": [],
   "source": [
    "! pip install -q lexical-diversity statsmodels pymorphy2[fast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M18BDn3sXuhc",
    "outputId": "67ff7919-c4c8-4236-f90d-35fe9387b4f1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "from lexical_diversity import lex_div as ld\n",
    "from nltk import word_tokenize\n",
    "from nltk import download as nltk_download\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pymorphy2\n",
    "\n",
    "from itertools import chain\n",
    "from typing import List, Set, MutableMapping, Optional, Sequence, Tuple, Union\n",
    "from enum import Flag, auto\n",
    "from string import punctuation\n",
    "\n",
    "nltk_download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyBLveLntJxn"
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiHI4_QVViDV"
   },
   "outputs": [],
   "source": [
    "dev = pd.read_csv('dev.tsv', sep='\\t')\n",
    "test = pd.read_csv('test.tsv', sep='\\t')\n",
    "ans = pd.read_csv('assignments.tsv', sep='\\t')\n",
    "\n",
    "ans = ans[ans['GOLDEN:class'].isna()]\n",
    "ans = ans[~ans['OUTPUT:q_1_error']]\n",
    "ans = ans[~ans['OUTPUT:q_2_error']]\n",
    "\n",
    "ans = ans[['INPUT:question_1_id', 'INPUT:question_2_id', 'OUTPUT:class', 'ASSIGNMENT:worker_id']].drop_duplicates()\n",
    "ans = ans.rename(columns={\n",
    "    'INPUT:question_1_id': 'left_id',\n",
    "    'INPUT:question_2_id': 'right_id',\n",
    "    'OUTPUT:class': 'class',\n",
    "    'ASSIGNMENT:worker_id': 'worker_id'\n",
    "})\n",
    "\n",
    "dev['key'] = dev['left_id'].apply(lambda x: str(x)) + '_' + dev['right_id'].apply(lambda x: str(x))\n",
    "test['key'] = test['left_id'].apply(lambda x: str(x)) + '_' + test['right_id'].apply(lambda x: str(x))\n",
    "ans['key'] = ans['left_id'].apply(lambda x: str(x)) + '_' + ans['right_id'].apply(lambda x: str(x))\n",
    "\n",
    "dev_ans = ans[ans['key'].isin(dev['key'])]\n",
    "test_ans = ans[ans['key'].isin(test['key'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SaY1oRz1Ztj"
   },
   "source": [
    "Estimating human baseline metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yM97Nr9E1dRk",
    "outputId": "1f982453-9660-4a44-dd96-c6f0c36e08d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human baseline\n",
      "\tAccuracy: 0.8434\n",
      "\tF1: 0.8660\n"
     ]
    }
   ],
   "source": [
    "ctrl_ans = pd.read_csv('assignments.tsv', sep='\\t')\n",
    "ctrl_ans = ctrl_ans[~ctrl_ans['GOLDEN:class'].isna()]\n",
    "human_preds = ctrl_ans['OUTPUT:class'].to_numpy()\n",
    "labels = ctrl_ans['GOLDEN:class'].to_numpy()both\n",
    "print('Human baseline')\n",
    "print('\\tAccuracy: {:.4f}'.format(accuracy_score(labels, human_preds)))\n",
    "print('\\tF1: {:.4f}'.format(f1_score(labels, human_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUXi0DtIcAKG"
   },
   "source": [
    "Count Interrater Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gf0V0xYpb_nO"
   },
   "outputs": [],
   "source": [
    "def fleiss_kappa_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    neg_cnt = (df['class'] == 0).astype(np.long).sum()\n",
    "    pos_cnt = (df['class'] == 1).astype(np.long).sum()\n",
    "    return pd.DataFrame({\n",
    "        'pos_cnt': [pos_cnt],\n",
    "        'neg_cnt': [neg_cnt],\n",
    "        'num_raters': [neg_cnt + pos_cnt]\n",
    "    })\n",
    "\n",
    "def fleiss_kappa_agg(df: pd.DataFrame) -> float:\n",
    "    table = df[['neg_cnt', 'pos_cnt']].to_numpy(dtype=np.long, copy=True)\n",
    "    return fleiss_kappa(table)\n",
    "\n",
    "def set_fleiss_kappa(df: pd.DataFrame, final_agg: bool = True) -> Union[float, pd.DataFrame]:\n",
    "    table = df.groupby('key').apply(fleiss_kappa_table).reset_index()\n",
    "    grouped = table.groupby('num_raters')\n",
    "\n",
    "    fleiss_agg = grouped.apply(fleiss_kappa_agg).reset_index()\n",
    "    fleiss_agg = fleiss_agg.rename(columns={0: 'fleiss_kappa'})\n",
    "\n",
    "    cnt = grouped.key.agg('count').reset_index()\n",
    "    cnt = cnt.rename(columns={'key': 'num_entities'})\n",
    "\n",
    "    aggregated = pd.merge(fleiss_agg, cnt, left_on='num_raters', right_on='num_raters')\n",
    "    if final_agg:\n",
    "        w = aggregated['num_entities'].to_numpy(dtype=np.long)\n",
    "        w = w / w.sum()\n",
    "        coefs = aggregated['fleiss_kappa'].to_numpy()\n",
    "        return (w * coefs).sum().item()\n",
    "    else:\n",
    "        return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhnQeD4hz6OH",
    "outputId": "1bd6164b-3861-4708-99f1-c0b317cb6ba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set inter-rater reliability: 0.7744\n",
      "Dev set inter-rater reliability: 0.6314\n"
     ]
    }
   ],
   "source": [
    "print('Test set inter-rater reliability: {:.4f}'.format(set_fleiss_kappa(test_ans)))\n",
    "print('Dev set inter-rater reliability: {:.4f}'.format(set_fleiss_kappa(dev_ans)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GK5vDadtDZs"
   },
   "source": [
    "Morphological stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QP58ihMm7-9"
   },
   "outputs": [],
   "source": [
    "def get_corpus_lemmas(df: pd.DataFrame) -> List[str]:\n",
    "    text_it = zip(df['left_text'], df['right_text'])\n",
    "    text_it = chain.from_iterable(text_it)\n",
    "    text_it = map(lambda sent: word_tokenize(sent, language='russian'), text_it)\n",
    "    text_it = chain.from_iterable(text_it)\n",
    "    text_it = map(lambda token: morph.parse(token)[0], text_it)\n",
    "    text_it = filter(lambda p: 'PNCT' not in p.tag, text_it)\n",
    "    text_it = map(lambda p: p.normal_form, text_it)\n",
    "    return list(text_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWPEUuF3tMKq"
   },
   "outputs": [],
   "source": [
    "punct_set = set(punctuation)\n",
    "\n",
    "def bag_of_tags(sent: str, tokens: Optional[Sequence[str]] = None) -> Set[str]:\n",
    "    if tokens is None:\n",
    "        tokens = word_tokenize(sent, language='russian')\n",
    "    parsed = map(lambda tok: morph.parse(tok)[0], tokens)\n",
    "    parsed = map(lambda p: str(p.tag).split(','), parsed)\n",
    "    parsed = chain.from_iterable(parsed)\n",
    "    return set(parsed)\n",
    "\n",
    "class Prop(Flag):\n",
    "    ENG = auto()\n",
    "    NUMBR = auto()\n",
    "    NUM_WORD = auto()\n",
    "    GRND = auto()\n",
    "    NEGATION = auto()\n",
    "    DBL_NEGATION = auto()\n",
    "    COMP_SENT = auto()\n",
    "\n",
    "def analyze(sent: str) -> Tuple[int, Prop]:\n",
    "    tokens = word_tokenize(sent, language='russian')\n",
    "\n",
    "    word_toks = filter(lambda token: len(set(token) & punct_set) == 0, tokens)\n",
    "    sent_len = len(list(word_toks))\n",
    "    tags = bag_of_tags(sent, tokens=tokens)\n",
    "    properties = Prop.GRND & Prop.NUMBR\n",
    "    if 'LATN' in tags:\n",
    "        properties |= Prop.ENG\n",
    "    if len({'NUMB', 'intg', 'real', 'ROMN'} & tags) > 0:\n",
    "        properties |= Prop.NUMBR\n",
    "    if 'NUMR' in tags:\n",
    "        properties |= Prop.NUM_WORD\n",
    "    if 'GRND' in tags:\n",
    "        properties |= Prop.GRND\n",
    "    \n",
    "    morphs = [morph.parse(tok)[0] for tok in tokens]\n",
    "    neg_cnt = 0\n",
    "    for p in morphs:\n",
    "        if p.tag.POS == 'PRCL' and p.normal_form in ('не', 'ни'):\n",
    "            neg_cnt += 1\n",
    "        if p.tag.POS in ('ADVB', 'NPRO') and p.normal_form.startswith('ни'):\n",
    "            neg_cnt += 1\n",
    "        if p.tag.POS == 'PRED' and p.normal_form.startswith('не'):\n",
    "            neg_cnt += 1\n",
    "        \n",
    "    if neg_cnt > 0:\n",
    "        properties |= Prop.NEGATION\n",
    "    if neg_cnt == 2:\n",
    "        properties |= Prop.DBL_NEGATION\n",
    "    return sent_len, properties\n",
    "\n",
    "def corpus_stat(df: pd.DataFrame):\n",
    "    len_s = 0\n",
    "    len_diff_s = 0\n",
    "    cnt = 0\n",
    "\n",
    "    stats = {item.name:0 for item in list(Prop)}\n",
    "\n",
    "    for left, right in zip(df['left_text'], df['right_text']):\n",
    "        left_len, left_props = analyze(left)\n",
    "        right_len, right_props = analyze(right)\n",
    "        len_s += left_len + right_len\n",
    "        len_diff_s += abs(left_len - right_len)\n",
    "        props = left_props | right_props\n",
    "        for item in list(Prop):\n",
    "            if item in props:\n",
    "                stats[item.name] += 1\n",
    "        cnt += 1\n",
    "    stats = {key: val / cnt for key, val in stats.items()}\n",
    "    stats['mean_len'] = len_s / (2 * cnt)\n",
    "    stats['mean_diff'] = len_diff_s / cnt\n",
    "    stats['pos_frac'] = df['class'].astype(np.float).mean()\n",
    "    lemmas = get_corpus_lemmas(df)\n",
    "    stats['diversity'] = ld.mtld(lemmas)\n",
    " \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9kw2uSs9Hvb",
    "outputId": "e98e1669-40bd-4376-f52e-45693f445518"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMP_SENT': 0.0,\n",
       " 'DBL_NEGATION': 0.005,\n",
       " 'ENG': 0.0,\n",
       " 'GRND': 0.03166666666666667,\n",
       " 'NEGATION': 0.085,\n",
       " 'NUMBR': 0.115,\n",
       " 'NUM_WORD': 0.0,\n",
       " 'diversity': 25.10119964835842,\n",
       " 'mean_diff': 1.335,\n",
       " 'mean_len': 8.034166666666666,\n",
       " 'pos_frac': 0.5}"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_stat(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0z3C_9M97jG3",
    "outputId": "d92141ae-02d4-4b46-d3c1-7e3a12d4452f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMP_SENT': 0.0,\n",
       " 'DBL_NEGATION': 0.010704727921498661,\n",
       " 'ENG': 0.0,\n",
       " 'GRND': 0.031222123104371096,\n",
       " 'NEGATION': 0.10526315789473684,\n",
       " 'NUMBR': 0.14540588760035683,\n",
       " 'NUM_WORD': 0.0008920606601248885,\n",
       " 'diversity': 31.89700841316214,\n",
       " 'mean_diff': 1.4424620874219447,\n",
       " 'mean_len': 8.085191793041927,\n",
       " 'pos_frac': 0.23996431757359502}"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_stat(dev)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "corpus_stats.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
