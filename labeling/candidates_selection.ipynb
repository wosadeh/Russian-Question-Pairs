{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMCbRoXS5I7a"
   },
   "outputs": [],
   "source": [
    "model_name = 'DeepPavlov/rubert-base-cased-sentence'\n",
    "BATCH_SIZE = 32\n",
    "SIMILARITY_THRESHOLD = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POu-7i0HhTuu"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73Tetxd-aYco",
    "outputId": "391035a4-1b9b-4ae4-afb0-97d5691d56d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 1.8MB 8.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 890kB 43.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.2MB 43.9MB/s \n",
      "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "! pip install -q transformers\n",
    "! pip install -q persist-queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-zZxcF4hXTm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchtext import data\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from persistqueue import Queue\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "from os import path, remove\n",
    "from shutil import copyfile, rmtree\n",
    "from typing import Callable, Optional, List, Tuple, Collection, Union\n",
    "from copy import copy\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MN97GYZm7JyF"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0VnDMtQF9vQ"
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33Nx0vP1TWFB"
   },
   "outputs": [],
   "source": [
    "# Source https://github.com/pytorch/pytorch/issues/11202\n",
    "def cosine_similarity(x1, x2=None, eps=1e-8):\n",
    "    x2 = x1 if x2 is None else x2\n",
    "    w1 = x1.norm(p=2, dim=1, keepdim=True)\n",
    "    w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True)\n",
    "    return torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpBTU8NRiH-Z"
   },
   "source": [
    "# Pairs distance heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SCcyzaEkbnhX",
    "outputId": "3f7287b9-61c9-4d5a-e57b-8d2c53f6be2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 890kB 14.7MB/s \n",
      "\u001b[K     |████████████████████████████████| 102kB 11.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
      "\u001b[?25h  Building wheel for pyxDamerauLevenshtein (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "! pip install -q \"textdistance[extras]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySzBHZf9P-A7",
    "outputId": "bc213f0f-dfe7-4cb4-d930-1452e1b8ce5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textdistance import levenshtein, bag\n",
    "from nltk import word_tokenize, wordpunct_tokenize\n",
    "from nltk import download as nltk_download\n",
    "nltk_download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TEt-BFep2_X"
   },
   "outputs": [],
   "source": [
    "class QuestionPairBase(object):\n",
    "    \"\"\"\n",
    "    Class for question pairs\n",
    "    Two pairs are equal, if they have small Levenshtein distance\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 left_id: int, left_text: str,\n",
    "                 right_id: int, right_text: str,\n",
    "                 similarity: float):\n",
    "        self.left_id = left_id\n",
    "        self.left_text = left_text\n",
    "        self.right_id = right_id\n",
    "        self.right_text = right_text\n",
    "        self.similarity = similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def _question_eq(left: str, right: str) -> bool:\n",
    "        return left == right\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        left_left_eq = self._question_eq(self.left_text, other.left_text)\n",
    "        right_right_eq = self._question_eq(self.right_text, other.right_text)\n",
    "\n",
    "        if left_left_eq and right_right_eq:\n",
    "            return True\n",
    "\n",
    "        left_right_eq = self._question_eq(self.left_text, other.right_text)\n",
    "        right_left_eq = self._question_eq(self.right_text, other.left_text)\n",
    "        if left_right_eq and right_left_eq:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __ne__(self, other) -> bool:\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    def __hash__(self):\n",
    "        # order agnostic hash\n",
    "        return self.left_text.__hash__() ^ self.right_text.__hash__()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6krCBIcKK8Os"
   },
   "outputs": [],
   "source": [
    "def create_heuristic_comparator(eq_heuristic: Callable[[str, str], bool]):\n",
    "    class QuestionPair(QuestionPairBase):\n",
    "        @staticmethod\n",
    "        def _question_eq(left: str, right: str) -> bool:\n",
    "            return eq_heuristic(left, right)\n",
    "\n",
    "    return QuestionPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNfg2BfYLfRW"
   },
   "outputs": [],
   "source": [
    "LevCharCmp = create_heuristic_comparator(\n",
    "    lambda s1, s2: levenshtein(s1, s2) <= 5\n",
    "    )\n",
    "\n",
    "def cmp_heuristic(left: str, right: str) -> bool:\n",
    "    # Compare two questions, using several heuristics\n",
    "\n",
    "    # compare in lower case\n",
    "    left = left.lower()\n",
    "    right = right.lower()\n",
    "\n",
    "    if levenshtein(left, right) <= 4:\n",
    "        # Two questions are very close (as symbol sequence)\n",
    "        return True\n",
    "\n",
    "    tok_left = word_tokenize(left)\n",
    "    tok_right = word_tokenize(right)\n",
    "\n",
    "    bow_dist = bag(tok_left, tok_right)\n",
    "    if bow_dist > 2:\n",
    "        # Two questions contains different words\n",
    "        return False\n",
    "    \n",
    "    lev_tok_dist = levenshtein(tok_left, tok_right)\n",
    "    if lev_tok_dist <= 2:\n",
    "        # Too close tokens sequence.\n",
    "        # Two questions are equal with respect of some noise\n",
    "        return True\n",
    "\n",
    "    # May be a transposition or easy but meaningful paraphrase\n",
    "    return False\n",
    "\n",
    "QuestionPair = create_heuristic_comparator(cmp_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie0iZS8chFF-"
   },
   "outputs": [],
   "source": [
    "def construct_similarity_dataframe(\n",
    "    it: Collection[Tuple[int, int, float]]\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "    questions_df = pd.read_csv(ROOT_DIR + 'questions.csv', sep=';')\n",
    "\n",
    "    pairs = set()\n",
    "\n",
    "    for left_id, right_id, sim_val in tqdm(it, unit='pair'):\n",
    "        left_text = questions_df.loc[questions_df['id'] == left_id, 'text'].item()\n",
    "        right_text = questions_df.loc[questions_df['id'] == right_id, 'text'].item()\n",
    "\n",
    "        # There are some pairs with too similar text\n",
    "        # Ignore too close (Levenshtein distance) pairs\n",
    "\n",
    "        if cmp_heuristic(left_text, right_text):\n",
    "            continue\n",
    "\n",
    "        pair_obj = QuestionPair(\n",
    "            left_id,\n",
    "            left_text,\n",
    "            right_id,\n",
    "            right_text,\n",
    "            sim_val\n",
    "        )\n",
    "        pairs.add(pair_obj)\n",
    "\n",
    "    n_pairs = len(pairs)\n",
    "    sim_df = pd.DataFrame({\n",
    "        'left_id': [-1] * n_pairs,\n",
    "        'left_text': [''] * n_pairs,\n",
    "        'left_url': [''] * n_pairs,\n",
    "        'right_id': [-1] * n_pairs,\n",
    "        'right_text': [''] * n_pairs,\n",
    "        'right_url': [''] * n_pairs,\n",
    "        'similarity': [0.] * n_pairs\n",
    "        })\n",
    "\n",
    "    for i, p in enumerate(pairs):\n",
    "        left_url = questions_df.loc[questions_df['id'] == p.left_id, 'url'].item()\n",
    "        right_url = questions_df.loc[questions_df['id'] == p.right_id, 'url'].item()\n",
    "        sim_df.at[i, 'left_url'] = left_url\n",
    "        sim_df.at[i, 'right_url'] = right_url\n",
    "\n",
    "        sim_df.at[i, 'left_text'] = p.left_text\n",
    "        sim_df.at[i, 'right_text'] = p.right_text\n",
    "        sim_df.at[i, 'left_id'] = p.left_id\n",
    "        sim_df.at[i, 'right_id'] = p.right_id\n",
    "        sim_df.at[i, 'similarity'] = p.similarity\n",
    "\n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sza8XhqAqqDs"
   },
   "outputs": [],
   "source": [
    "def sim_dataframe_from_queue(queue: Queue) -> pd.DataFrame:\n",
    "    class QueueIter(object):\n",
    "        def __init__(self, queue: Queue):\n",
    "            self.queue = queue\n",
    "            self.size = self.queue.qsize()\n",
    "            self.remain = copy(self.size)\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.size\n",
    "\n",
    "        def __iter__(self):\n",
    "            return self\n",
    "\n",
    "        def __next__(self):\n",
    "            if self.remain > 0:\n",
    "                self.remain -= 1\n",
    "                return self.queue.get()\n",
    "            else:\n",
    "                raise StopIteration\n",
    "\n",
    "    seq_obj = QueueIter(queue)\n",
    "    return construct_similarity_dataframe(seq_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5EOwnoN_0Ge"
   },
   "source": [
    "# BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogRTYJY1OOJp"
   },
   "source": [
    "## Make up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVAvpqIFjJrw"
   },
   "outputs": [],
   "source": [
    "class HuggingFaceField(data.Field):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(\n",
    "            tokenize=tokenizer.tokenize,\n",
    "            use_vocab=False,\n",
    "            pad_token=tokenizer.pad_token,\n",
    "            init_token=tokenizer.cls_token\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def numericalize(self, arr, device):\n",
    "        arr = [self.tokenizer.convert_tokens_to_ids(x) for x in arr]\n",
    "        return torch.LongTensor(arr).to(device)\n",
    "\n",
    "class LabelField(data.Field):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            use_vocab=False,\n",
    "            sequential=False,\n",
    "            tokenize=lambda x: x)\n",
    "        \n",
    "    def numericalize(self, arr, device):\n",
    "        arr = [int(item) for item in arr]\n",
    "        return torch.LongTensor(arr).to(device)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeO_FSPShnvm"
   },
   "source": [
    "Loading CSV file with questions. It supposed that file contains 'id' (the question unique id in dataset) and 'text' (text of the question) columns.\n",
    "\n",
    "Supposed that `ROOT_DIR` variable stores path to directory with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAZRTuAJ3zJ-"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pad_index = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "unk_index = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "cls_index = tokenizer.convert_tokens_to_ids(tokenizer.cls_token)\n",
    "sep_index = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP219LS-9URW"
   },
   "source": [
    "Remove already processed items from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dH7DiK659ZNj"
   },
   "outputs": [],
   "source": [
    "if path.isfile(ROOT_DIR + 'done_ids.txt'):\n",
    "    with open(ROOT_DIR + 'done_ids.txt') as f:\n",
    "        done = set(int(v) for v in f.read().rstrip().split())\n",
    "\n",
    "    df = pd.read_csv(ROOT_DIR + 'questions.csv', sep=';')\n",
    "    df = df[~df['id'].isin(done)]\n",
    "    df.to_csv('./questions.csv', index=False, sep=';')\n",
    "else:\n",
    "    copyfile(ROOT_DIR + 'questions.csv', './questions.csv')\n",
    "    done = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uw16RUOx5TuD"
   },
   "outputs": [],
   "source": [
    "TEXT = HuggingFaceField(tokenizer)\n",
    "ID = LabelField()\n",
    "dataset = data.TabularDataset(\n",
    "    'questions.csv',\n",
    "    'CSV',\n",
    "    {'text': ('text', TEXT), 'id': ('id', ID)},\n",
    "    skip_header=False,\n",
    "    csv_reader_params={'delimiter': ';'}\n",
    ")\n",
    "\n",
    "iterator = data.BucketIterator(\n",
    "    dataset,\n",
    "    BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    device=device,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVESZa8s7uek"
   },
   "source": [
    "Initializing file to store intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxRh0HT67942"
   },
   "outputs": [],
   "source": [
    "embed_queue = Queue(ROOT_DIR + 'embed.queue', autosave=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFaYV_O2OLdF"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyQQxawV_5vR"
   },
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca6IlVosACqP"
   },
   "outputs": [],
   "source": [
    "with open(ROOT_DIR + 'done_ids.txt', 'a') as done_f:\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in tqdm(iterator, unit='batch'):\n",
    "            tokens = batch.text\n",
    "            mask = (tokens != pad_index).float()\n",
    "\n",
    "            output = model(\n",
    "                tokens,\n",
    "                attention_mask=mask,\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            token_embeddings = output['last_hidden_state']\n",
    "\n",
    "            # Mean-pooling for sentence embedding.\n",
    "            # The same maner as in original Sentence-BERT work\n",
    "            sent_embeddings = torch.sum(token_embeddings * mask.unsqueeze(-1),\n",
    "                                        dim=1)\n",
    "            mask_sum = torch.sum(mask, dim=1, keepdim=True)\n",
    "            sent_embeddings = sent_embeddings / mask_sum\n",
    "\n",
    "            # Saving embeddings to queue and add processed id to file\n",
    "            for (sent_id, emb) in zip(batch.id.cpu(), sent_embeddings.cpu()):\n",
    "                sent_id = int(sent_id)\n",
    "                embed_queue.put((sent_id, emb))\n",
    "\n",
    "                done_f.write(str(sent_id) + ' ')\n",
    "                done.add(sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqF6UFfsI_QU"
   },
   "outputs": [],
   "source": [
    "questions_embeddings = {}\n",
    "q_size = embed_queue.qsize()\n",
    "for _ in range(q_size):\n",
    "    sent_id, emb_vec = embed_queue.get()\n",
    "    questions_embeddings[sent_id] = emb_vec.numpy()\n",
    "\n",
    "with open(ROOT_DIR + 'embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(questions_embeddings, f)\n",
    "\n",
    "assert embed_queue.empty()\n",
    "assert len(questions_embeddings) == q_size\n",
    "\n",
    "del embed_queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2cXzsalIC7R"
   },
   "source": [
    "# Embeddings similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe41nLJ0LDvV"
   },
   "source": [
    "Explicit similarity matrix wiil be too huge to fit the memory.\n",
    "\n",
    "To overcome this limit, all embeddings will be splitted on batches. Similarity matrix will be calculated between every batch pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ne1BUjBmv-2n"
   },
   "outputs": [],
   "source": [
    "if not 'questions_embeddings' in locals() and not 'questions_embeddings' in globals():\n",
    "    with open(ROOT_DIR + 'embeddings.pkl', 'rb') as f:\n",
    "        questions_embeddings = pickle.load(f)\n",
    "\n",
    "# Sort dataset ids to reproduce order, if calculation is interrupted\n",
    "dataset_ids = np.array(sorted(questions_embeddings.keys()))\n",
    "emb_dim = next(iter(questions_embeddings.values())).shape[0]\n",
    "embeddings = np.empty((len(questions_embeddings), emb_dim), dtype=np.float)\n",
    "\n",
    "for i, sent_id in enumerate(dataset_ids):\n",
    "    embeddings[i] = questions_embeddings[sent_id]\n",
    "\n",
    "del questions_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-e5bgahMEjf"
   },
   "outputs": [],
   "source": [
    "SIM_BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEktgxHAPO2m"
   },
   "outputs": [],
   "source": [
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XxEMsxfwSmv"
   },
   "outputs": [],
   "source": [
    "embeddings = torch.from_numpy(embeddings).detach().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8S0HZyKMDbm"
   },
   "outputs": [],
   "source": [
    "sim_pairs_queue = Queue(ROOT_DIR + 'sim.queue', autosave=False)\n",
    "\n",
    "batch_start_idx = list(range(0, embeddings.size(0), SIM_BATCH_SIZE))\n",
    "\n",
    "# Load last finished indexes\n",
    "if not path.isfile(ROOT_DIR + 'sim_last_iter.txt'):\n",
    "    with open(ROOT_DIR + 'sim_last_iter.txt', 'w') as f:\n",
    "        f.write('-1 ' + str(len(batch_start_idx) - 1) + ' 0\\n')\n",
    "\n",
    "with open(ROOT_DIR + 'sim_last_iter.txt') as f:\n",
    "    last_i, last_j, n_found = [int(v) for v in f.readline().rstrip().split()]\n",
    "\n",
    "if last_j + 1 >= len(batch_start_idx):\n",
    "    start_i = last_i + 1\n",
    "    start_j = start_i\n",
    "else:\n",
    "    start_i = last_i\n",
    "    start_j = last_j + 1\n",
    "\n",
    "start_i = max(0, start_i)\n",
    "start_j = max(start_j, start_i)\n",
    "\n",
    "# Calculate info for progress bar\n",
    "total_pairs = len(batch_start_idx) * (len(batch_start_idx) + 1) // 2\n",
    "init_pairs = start_i * (2 * len(batch_start_idx) - start_i + 1) // 2\n",
    "init_pairs += start_j - start_i\n",
    "\n",
    "with open(ROOT_DIR + 'sim_last_iter.txt', 'r+') as last_it_f, torch.no_grad():\n",
    "    with tqdm(total=total_pairs, unit='batch pair', initial=init_pairs) as pbar:\n",
    "        for i in range(start_i, len(batch_start_idx)):\n",
    "            batch1_start = batch_start_idx[i]\n",
    "            batch1 = embeddings[batch1_start: batch1_start + SIM_BATCH_SIZE]\n",
    "\n",
    "            for j in range(max(i, start_j), len(batch_start_idx)):\n",
    "                start_j = -1\n",
    "\n",
    "                batch2_start = batch_start_idx[j]\n",
    "                batch2 = embeddings[batch2_start: batch2_start + SIM_BATCH_SIZE]\n",
    "\n",
    "                sim_matrix = cosine_similarity(batch1, batch2).cpu()\n",
    "                if i == j:\n",
    "                    sim_matrix = torch.triu(sim_matrix, diagonal=1)\n",
    "\n",
    "                # Using calculated batch-batch similarity matrix \n",
    "                # recover corresponding ids in dataset\n",
    "                idx = torch.where(sim_matrix >= SIMILARITY_THRESHOLD)\n",
    "                for batch1_shift, batch2_shift in zip(*idx):\n",
    "                    left_id = batch1_start + batch1_shift.item()\n",
    "                    right_id = batch2_start + batch2_shift.item()\n",
    "\n",
    "                    left_id = dataset_ids[left_id].item()\n",
    "                    right_id = dataset_ids[right_id].item()\n",
    "                    sim_val = sim_matrix[batch1_shift, batch2_shift].item()\n",
    "\n",
    "                    # Save close pair to queue\n",
    "                    sim_pairs_queue.put((left_id, right_id, sim_val))\n",
    "                    n_found += 1\n",
    "\n",
    "                # Using try clase to log progress, \n",
    "                # even if execution was interuppted by Ctrl-C\n",
    "                try:\n",
    "                    pass\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "                finally:\n",
    "                    # Update progress\n",
    "                    last_it_f.seek(0)\n",
    "                    last_it_f.truncate(0)\n",
    "                    last_it_f.write(f'{i} {j} {n_found}\\n')\n",
    "                    pbar.update()\n",
    "                pbar.set_postfix({'similar pairs found': n_found})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6OxgwH_Og4Z"
   },
   "source": [
    "Save the resulting pairs in CSV table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CY8kElQyCQVf",
    "outputId": "13eedab0-3e41-443b-a187-64e3581eb49b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default tempdir '/tmp/tmpqxhqy0hj' is not on the same filesystem with queue path '/content/drive/MyDrive/ParaPhrase/Unsupervised embeddings/sim.queue',defaulting to '/content/drive/MyDrive/ParaPhrase/Unsupervised embeddings/sim.queue'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254748\n"
     ]
    }
   ],
   "source": [
    "if 'sim_pairs_queue' not in locals() and 'sim_pairs_queue' not in globals(): \n",
    "    sim_pairs_queue = Queue(ROOT_DIR + 'sim.queue', autosave=False)\n",
    "    print(sim_pairs_queue.qsize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "b95058c9e6a947d2a3442cb9f86c33d3",
      "d56ddca379984b87bf7636f8c789bc0d",
      "350d7b53a35c4ad6af695f063b1db2a7",
      "3484ee2791f34e63beed3495bbb48e0d",
      "cb45199ce6de472987dcc9149f6d9421",
      "00738901051d47488b932c68f9098fed",
      "ca96dc5754c5489daad13eeb4c08a169",
      "6fe30f63f6d84261bb89a8208110a8ac"
     ]
    },
    "id": "qrt-JOWmxksb",
    "outputId": "4f6ad4b5-4c2a-42a6-f102-b95f9eac1bd8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95058c9e6a947d2a3442cb9f86c33d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=254748.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sim_df = sim_dataframe_from_queue(sim_pairs_queue)\n",
    "sim_df = sim_df.sort_values(by=['similarity'], ascending=False)\n",
    "\n",
    "sim_df.to_csv(ROOT_DIR + 'sim.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2G5_iHGeFsY"
   },
   "source": [
    "# Similarity bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C0UlG60eLcR"
   },
   "source": [
    "In this section pairs with different similarity values are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6UWcJwzgNS4"
   },
   "outputs": [],
   "source": [
    "SIM_BATCH_SIZE = 1024\n",
    "BIN_SIZE = 50\n",
    "CROWD_OUT_PROB = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MYdbRbGoIXY"
   },
   "outputs": [],
   "source": [
    "def calc_bins(\n",
    "    embeddings: torch.Tensor,\n",
    "    dataset_ids: torch.LongTensor\n",
    "    ) -> List[List[Tuple[int, int, float]]]:\n",
    "\n",
    "    bins = [[] for _ in range(10)]\n",
    "    batch_start_idx = list(range(0, embeddings.size(0), SIM_BATCH_SIZE))\n",
    "\n",
    "    # Collect more pair then necessary, because of possible identical pairs \n",
    "    bin_size = BIN_SIZE * 3 // 2\n",
    "\n",
    "    # Calculate info for progress bar\n",
    "    total_pairs = len(batch_start_idx) * (len(batch_start_idx) + 1) // 2\n",
    "    pairs_pbar = tqdm(total=total_pairs, unit='batch', position=1, desc='All pairs processed')\n",
    "    bins_pbar = tqdm(total=100, position=0, desc=\"Bins progress\")\n",
    "\n",
    "    for i in range(len(batch_start_idx)):\n",
    "        batch1_start = batch_start_idx[i]\n",
    "        batch1 = embeddings[batch1_start: batch1_start + SIM_BATCH_SIZE]\n",
    "\n",
    "        for j in range(i, len(batch_start_idx)):\n",
    "            batch2_start = batch_start_idx[j]\n",
    "            batch2 = embeddings[batch2_start: batch2_start + SIM_BATCH_SIZE]\n",
    "\n",
    "            sim_matrix = cosine_similarity(batch1, batch2)\n",
    "            if i == j:\n",
    "                sim_matrix = torch.triu(sim_matrix, diagonal=1)\n",
    "\n",
    "            # Extend bins\n",
    "            min_bin_size = bin_size + 1\n",
    "            for bin_i in range(10):\n",
    "                bin_sim_min = 0.1 * bin_i\n",
    "                bin_sim_max = bin_sim_min + 0.1\n",
    "\n",
    "                idx = torch.where(\n",
    "                    (sim_matrix > bin_sim_min)\n",
    "                    &\n",
    "                    (sim_matrix <= bin_sim_max)\n",
    "                    )\n",
    "                \n",
    "                for batch1_shift, batch2_shift in zip(*idx):\n",
    "                    # Using calculated batch-batch similarity matrix \n",
    "                    # recover corresponding ids in dataset\n",
    "                    left_id = batch1_start + batch1_shift.item()\n",
    "                    right_id = batch2_start + batch2_shift.item()\n",
    "\n",
    "                    left_id = dataset_ids[left_id].item()\n",
    "                    right_id = dataset_ids[right_id].item()\n",
    "                    sim_val = sim_matrix[batch1_shift, batch2_shift].item()\n",
    "                    if len(bins[bin_i]) < bin_size:\n",
    "                        bins[bin_i].append((left_id, right_id, sim_val))\n",
    "                    elif np.random.random() <= CROWD_OUT_PROB:\n",
    "                        # Bin is completely filled\n",
    "                        k = np.random.randint(bin_size)\n",
    "                        bins[bin_i][k] = (left_id, right_id, sim_val)\n",
    "                \n",
    "                min_bin_size = min(min_bin_size, len(bins[bin_i]))\n",
    "\n",
    "            pairs_pbar.update()\n",
    "            bins_pbar.update(100 * min_bin_size // bin_size - bins_pbar.n)\n",
    "            if min_bin_size >= bin_size:\n",
    "                pairs_pbar.close()\n",
    "                bins_pbar.close()\n",
    "                return bins\n",
    "    \n",
    "    pairs_pbar.close()\n",
    "    bins_pbar.close()\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4bzhqSJgNSq"
   },
   "outputs": [],
   "source": [
    "if not 'questions_embeddings' in locals() and not 'questions_embeddings' in globals():\n",
    "    with open(ROOT_DIR + 'embeddings.pkl', 'rb') as f:\n",
    "        questions_embeddings = pickle.load(f)\n",
    "\n",
    "# Sort dataset ids to reproduce order, if calculation is interrupted\n",
    "dataset_ids = np.array(sorted(questions_embeddings.keys()))\n",
    "emb_dim = next(iter(questions_embeddings.values())).shape[0]\n",
    "embeddings = np.empty((len(questions_embeddings), emb_dim), dtype=np.float)\n",
    "\n",
    "for i, sent_id in enumerate(dataset_ids):\n",
    "    embeddings[i] = questions_embeddings[sent_id]\n",
    "\n",
    "del questions_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-zNqA_UgNS6"
   },
   "outputs": [],
   "source": [
    "embeddings = torch.from_numpy(embeddings).detach().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3gJReZugNS7"
   },
   "outputs": [],
   "source": [
    "idx = torch.randperm(embeddings.shape[0])\n",
    "bins = calc_bins(embeddings[idx], dataset_ids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "d04fbd614f7c4cda9f43aab096ef1f22",
      "8df1614d829547a4b45f388e1c5ae237",
      "129c3b34396b4088a7070c7e462cb6a4",
      "144215337778441aba97859e40439e30",
      "af94049ca8ba43fbadc9e970e1d73a6b",
      "1718f93bc6c0458c9e7602f66f16267a",
      "0d7a19a34dd248f9a6ca1c586910224e",
      "63a7697326e144929937927bf61b8a55"
     ]
    },
    "id": "_k8YvLdcuXHl",
    "outputId": "fb07ada6-7859-4203-8052-42ba0f4ed9ef"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04fbd614f7c4cda9f43aab096ef1f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ext_bins_df = construct_similarity_dataframe(chain.from_iterable(bins))\n",
    "\n",
    "for i in range(10):\n",
    "    bin_sim_min = 0.1 * i\n",
    "    bin_sim_max = bin_sim_min + 0.1\n",
    "\n",
    "    bin = ext_bins_df.loc[(ext_bins_df.similarity > bin_sim_min) & (ext_bins_df.similarity <= bin_sim_max)]\n",
    "    if len(bin) < BIN_SIZE:\n",
    "        warnings.warn(f'i th bin is smaller then required size')\n",
    "    else:\n",
    "        bin = bin.sample(BIN_SIZE)\n",
    "\n",
    "    bin['bin'] = i\n",
    "    if i == 0:\n",
    "        bins_df = bin\n",
    "    else:\n",
    "        bins_df = pd.concat([bins_df, bin])\n",
    "bins_df = bins_df.sort_values(by=['bin'])\n",
    "bins_df.to_csv(ROOT_DIR + 'calibration_bins.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5qVoUHkIgSx"
   },
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woHSQZuJIno5"
   },
   "outputs": [],
   "source": [
    "rmtree(ROOT_DIR + 'embed.queue')\n",
    "remove(ROOT_DIR + 'done_ids.txt')\n",
    "\n",
    "rmtree(ROOT_DIR + 'sim.queue')\n",
    "remove(ROOT_DIR + 'sim_last_iter.txt')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kpBTU8NRiH-Z",
    "R2G5_iHGeFsY"
   ],
   "name": "Unsupervised embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00738901051d47488b932c68f9098fed": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d7a19a34dd248f9a6ca1c586910224e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "129c3b34396b4088a7070c7e462cb6a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1718f93bc6c0458c9e7602f66f16267a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af94049ca8ba43fbadc9e970e1d73a6b",
      "value": 1
     }
    },
    "144215337778441aba97859e40439e30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63a7697326e144929937927bf61b8a55",
      "placeholder": "​",
      "style": "IPY_MODEL_0d7a19a34dd248f9a6ca1c586910224e",
      "value": " 750/? [00:01&lt;00:00, 594.77pair/s]"
     }
    },
    "1718f93bc6c0458c9e7602f66f16267a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3484ee2791f34e63beed3495bbb48e0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fe30f63f6d84261bb89a8208110a8ac",
      "placeholder": "​",
      "style": "IPY_MODEL_ca96dc5754c5489daad13eeb4c08a169",
      "value": " 254748/254748 [22:34&lt;00:00, 188.06pair/s]"
     }
    },
    "350d7b53a35c4ad6af695f063b1db2a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_00738901051d47488b932c68f9098fed",
      "max": 254748,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb45199ce6de472987dcc9149f6d9421",
      "value": 254748
     }
    },
    "63a7697326e144929937927bf61b8a55": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fe30f63f6d84261bb89a8208110a8ac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8df1614d829547a4b45f388e1c5ae237": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af94049ca8ba43fbadc9e970e1d73a6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b95058c9e6a947d2a3442cb9f86c33d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_350d7b53a35c4ad6af695f063b1db2a7",
       "IPY_MODEL_3484ee2791f34e63beed3495bbb48e0d"
      ],
      "layout": "IPY_MODEL_d56ddca379984b87bf7636f8c789bc0d"
     }
    },
    "ca96dc5754c5489daad13eeb4c08a169": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb45199ce6de472987dcc9149f6d9421": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d04fbd614f7c4cda9f43aab096ef1f22": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_129c3b34396b4088a7070c7e462cb6a4",
       "IPY_MODEL_144215337778441aba97859e40439e30"
      ],
      "layout": "IPY_MODEL_8df1614d829547a4b45f388e1c5ae237"
     }
    },
    "d56ddca379984b87bf7636f8c789bc0d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
